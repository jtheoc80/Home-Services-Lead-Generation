#!/usr/bin/env python3
"""
Lead ingest module for importing CSV data into PostgreSQL.

This module reads the leads_recent.csv file generated by the permit_leads
export process and imports it into the PostgreSQL database.
"""

import os
import sys
import csv
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

import psycopg2
from psycopg2.extras import RealDictCursor

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LeadIngestor:
    def __init__(self, db_url: str):
        """Initialize ingestor with database connection."""
        self.db_url = db_url
        
    def connect_db(self):
        """Create database connection."""
        return psycopg2.connect(self.db_url)
    
    def parse_csv_row(self, row: Dict[str, str]) -> Dict[str, Any]:
        """Parse a CSV row and convert types appropriately."""
        parsed = {}
        
        # String fields (keep as-is or None for empty)
        string_fields = [
            'jurisdiction', 'permit_id', 'address', 'description', 'work_class',
            'category', 'status', 'applicant', 'owner', 'apn', 'land_use',
            'owner_kind', 'budget_band', 'scoring_version'
        ]
        
        for field in string_fields:
            value = row.get(field, '').strip()
            parsed[field] = value if value else None
            
        # Numeric fields
        numeric_fields = [
            'value', 'latitude', 'longitude', 'heated_sqft', 'lot_size',
            'lead_score', 'score_recency', 'score_trade_match', 'score_value',
            'score_parcel_age', 'score_inspection'
        ]
        
        for field in numeric_fields:
            value = row.get(field, '').strip()
            if value and value != 'None':
                try:
                    parsed[field] = float(value)
                except (ValueError, TypeError):
                    parsed[field] = None
            else:
                parsed[field] = None
                
        # Integer fields
        integer_fields = ['year_built']
        for field in integer_fields:
            value = row.get(field, '').strip()
            if value and value != 'None':
                try:
                    parsed[field] = int(float(value))  # handle case where it's "2020.0"
                except (ValueError, TypeError):
                    parsed[field] = None
            else:
                parsed[field] = None
                
        # Boolean fields
        bool_value = row.get('is_residential', '').strip().lower()
        if bool_value in ['true', '1', 'yes']:
            parsed['is_residential'] = True
        elif bool_value in ['false', '0', 'no']:
            parsed['is_residential'] = False
        else:
            parsed['is_residential'] = None
            
        # Date fields
        date_fields = ['issue_date', 'start_by_estimate']
        for field in date_fields:
            value = row.get(field, '').strip()
            if value and value != 'None':
                try:
                    # Try parsing various date formats
                    for fmt in ['%Y-%m-%d', '%m/%d/%Y', '%Y-%m-%d %H:%M:%S']:
                        try:
                            parsed[field] = datetime.strptime(value, fmt).date()
                            break
                        except ValueError:
                            continue
                    else:
                        parsed[field] = None
                except (ValueError, TypeError):
                    parsed[field] = None
            else:
                parsed[field] = None
                
        # Timestamp fields
        timestamp_value = row.get('scraped_at', '').strip()
        if timestamp_value and timestamp_value != 'None':
            try:
                # Try parsing various timestamp formats
                for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                    try:
                        parsed['scraped_at'] = datetime.strptime(timestamp_value, fmt)
                        break
                    except ValueError:
                        continue
                else:
                    parsed['scraped_at'] = None
            except (ValueError, TypeError):
                parsed['scraped_at'] = None
        else:
            parsed['scraped_at'] = None
            
        # Array fields (trade_tags)
        trade_tags_value = row.get('trade_tags', '').strip()
        if trade_tags_value and trade_tags_value != 'None':
            try:
                # Handle various formats: "['tag1', 'tag2']" or "tag1,tag2"
                if trade_tags_value.startswith('[') and trade_tags_value.endswith(']'):
                    # Remove brackets and split by comma, clean quotes
                    tags = trade_tags_value[1:-1].split(',')
                    parsed['trade_tags'] = [tag.strip().strip('\'"') for tag in tags if tag.strip()]
                else:
                    # Simple comma-separated
                    parsed['trade_tags'] = [tag.strip() for tag in trade_tags_value.split(',') if tag.strip()]
            except:
                parsed['trade_tags'] = None
        else:
            parsed['trade_tags'] = None
            
        return parsed
    
    def ingest_csv(self, csv_file_path: str) -> int:
        """
        Ingest leads from CSV file into PostgreSQL database.
        
        Returns:
            Number of records successfully ingested
        """
        if not os.path.exists(csv_file_path):
            raise FileNotFoundError(f"CSV file not found: {csv_file_path}")
            
        logger.info(f"Starting ingest from {csv_file_path}")
        
        conn = self.connect_db()
        try:
            cur = conn.cursor()
            
            # Prepare the INSERT statement
            insert_sql = """
                INSERT INTO leads (
                    jurisdiction, permit_id, address, description, work_class, category,
                    status, issue_date, applicant, owner, value, is_residential, scraped_at,
                    latitude, longitude, apn, year_built, heated_sqft, lot_size, land_use,
                    owner_kind, trade_tags, budget_band, start_by_estimate,
                    lead_score, score_recency, score_trade_match, score_value,
                    score_parcel_age, score_inspection, scoring_version
                ) VALUES (
                    %(jurisdiction)s, %(permit_id)s, %(address)s, %(description)s, %(work_class)s, %(category)s,
                    %(status)s, %(issue_date)s, %(applicant)s, %(owner)s, %(value)s, %(is_residential)s, %(scraped_at)s,
                    %(latitude)s, %(longitude)s, %(apn)s, %(year_built)s, %(heated_sqft)s, %(lot_size)s, %(land_use)s,
                    %(owner_kind)s, %(trade_tags)s, %(budget_band)s, %(start_by_estimate)s,
                    %(lead_score)s, %(score_recency)s, %(score_trade_match)s, %(score_value)s,
                    %(score_parcel_age)s, %(score_inspection)s, %(scoring_version)s
                )
                ON CONFLICT (jurisdiction, permit_id) 
                DO UPDATE SET
                    address = EXCLUDED.address,
                    description = EXCLUDED.description,
                    work_class = EXCLUDED.work_class,
                    category = EXCLUDED.category,
                    status = EXCLUDED.status,
                    issue_date = EXCLUDED.issue_date,
                    applicant = EXCLUDED.applicant,
                    owner = EXCLUDED.owner,
                    value = EXCLUDED.value,
                    is_residential = EXCLUDED.is_residential,
                    scraped_at = EXCLUDED.scraped_at,
                    latitude = EXCLUDED.latitude,
                    longitude = EXCLUDED.longitude,
                    apn = EXCLUDED.apn,
                    year_built = EXCLUDED.year_built,
                    heated_sqft = EXCLUDED.heated_sqft,
                    lot_size = EXCLUDED.lot_size,
                    land_use = EXCLUDED.land_use,
                    owner_kind = EXCLUDED.owner_kind,
                    trade_tags = EXCLUDED.trade_tags,
                    budget_band = EXCLUDED.budget_band,
                    start_by_estimate = EXCLUDED.start_by_estimate,
                    lead_score = EXCLUDED.lead_score,
                    score_recency = EXCLUDED.score_recency,
                    score_trade_match = EXCLUDED.score_trade_match,
                    score_value = EXCLUDED.score_value,
                    score_parcel_age = EXCLUDED.score_parcel_age,
                    score_inspection = EXCLUDED.score_inspection,
                    scoring_version = EXCLUDED.scoring_version,
                    updated_at = now()
            """
            
            records_processed = 0
            records_inserted = 0
            
            with open(csv_file_path, 'r', encoding='utf-8') as csvfile:
                reader = csv.DictReader(csvfile)
                
                for row in reader:
                    try:
                        parsed_row = self.parse_csv_row(row)
                        cur.execute(insert_sql, parsed_row)
                        records_processed += 1
                        
                        if records_processed % 100 == 0:
                            logger.info(f"Processed {records_processed} records...")
                            
                    except Exception as e:
                        logger.error(f"Error processing row {records_processed + 1}: {e}")
                        logger.error(f"Row data: {row}")
                        continue
                        
            conn.commit()
            
            # Get the count of records actually inserted/updated
            cur.execute("SELECT COUNT(*) FROM leads")
            total_records = cur.fetchone()[0]
            
            logger.info(f"Ingest completed successfully!")
            logger.info(f"Records processed: {records_processed}")
            logger.info(f"Total records in database: {total_records}")
            
            return records_processed
            
        except Exception as e:
            conn.rollback()
            logger.error(f"Error during ingest: {e}")
            raise
        finally:
            conn.close()


def main():
    """Main entry point for the ingest script."""
    if len(sys.argv) != 2:
        print("Usage: python -m backend.app.ingest <csv_file_path>")
        print("Example: python -m backend.app.ingest out/leads_recent.csv")
        sys.exit(1)
        
    csv_file_path = sys.argv[1]
    
    # Get database URL from environment
    db_url = os.environ.get('DATABASE_URL')
    if not db_url:
        logger.error("DATABASE_URL environment variable not set")
        sys.exit(1)
        
    try:
        ingestor = LeadIngestor(db_url)
        records_processed = ingestor.ingest_csv(csv_file_path)
        logger.info(f"Successfully ingested {records_processed} records")
        
    except Exception as e:
        logger.error(f"Ingest failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()