name: Nightly Pipeline

on:
  schedule:
    # Configurable cron schedule from environment variable, default to 5 AM UTC
    - cron: "0 5 * * *"
  workflow_dispatch:
    inputs:
      days:
        description: 'Number of days to scrape (default: 14)'
        required: false
        default: '14'
      force_run:
        description: 'Force run even if no recent changes'
        type: boolean
        default: false

env:
  CRON_SCRAPE_UTC: ${{ vars.CRON_SCRAPE_UTC || '0 5 * * *' }}

jobs:
  scrape_and_process:
    runs-on: ubuntu-latest
    
    env:
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      REGISTRY_PATH: config/registry.yaml
      LAUNCH_SCOPE: houston
      DEFAULT_REGION: tx-houston
      ALLOW_EXPORTS: false
      USE_ML_SCORING: false
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          
      - name: Install permit_leads dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r permit_leads/requirements.txt
          
      - name: Install backend dependencies  
        run: |
          pip install -r backend/requirements.txt
          
      - name: Create output directory
        run: |
          mkdir -p out
          
      - name: Run scraper for Houston counties only
        run: |
          # Use input parameter if provided, otherwise default to 14 days
          DAYS="${{ github.event.inputs.days || '14' }}"
          python permit_leads/main.py --days $DAYS --outdir out
          
      - name: Export leads to CSV
        run: |
          python permit_leads/export_leads.py
          
      - name: Setup database schema
        if: env.DATABASE_URL != ''
        run: |
          python -c "
          import psycopg2
          import os
          
          if not os.environ.get('DATABASE_URL'):
              print('DATABASE_URL not set, skipping database operations')
              exit(0)
          
          # Read and execute the schema
          with open('backend/app/models.sql', 'r') as f:
              schema_sql = f.read()
          
          # Connect to database and execute schema
          conn = psycopg2.connect(os.environ['DATABASE_URL'])
          cur = conn.cursor()
          cur.execute(schema_sql)
          conn.commit()
          cur.close()
          conn.close()
          print('Schema setup completed')
          "
          
      - name: Ingest leads to PostgreSQL
        if: env.DATABASE_URL != '' && hashFiles('out/leads_recent.csv') != ''
        run: |
          python backend/app/ingest.py out/leads_recent.csv
          
      - name: Process notifications (if enabled)
        if: env.DATABASE_URL != ''
        run: |
          python -c "
          import os
          import sys
          
          if not os.environ.get('DATABASE_URL'):
              print('DATABASE_URL not set, skipping notification processing')
              sys.exit(0)
              
          # Import notification processing from backend
          sys.path.append('backend/app')
          
          try:
              from utils.notifications import process_new_lead_notifications
              
              # Process notifications for new leads
              processed_count = process_new_lead_notifications()
              print(f'Processed {processed_count} notifications')
              
          except ImportError as e:
              print(f'Notification processing not available: {e}')
          except Exception as e:
              print(f'Error processing notifications: {e}')
          "
          
      - name: Upload leads artifact (audit only)
        uses: actions/upload-artifact@v4
        with:
          name: leads-recent-${{ github.run_number }}
          path: out/leads_recent.csv
          if-no-files-found: warn
          retention-days: 30
          
      - name: Upload processing logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: processing-logs-${{ github.run_number }}
          path: |
            out/*.log
            *.log
          if-no-files-found: ignore
          retention-days: 7
          
      - name: Report run summary
        if: always()
        run: |
          echo "## Nightly Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Scope**: Houston counties only" >> $GITHUB_STEP_SUMMARY
          echo "- **Days scraped**: ${{ github.event.inputs.days || '14' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run time**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "out/leads_recent.csv" ]; then
            LEAD_COUNT=$(tail -n +2 out/leads_recent.csv | wc -l)
            echo "- **Leads processed**: $LEAD_COUNT" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Leads processed**: 0 (no CSV generated)" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "- **Database URL configured**: ${{ env.DATABASE_URL != '' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Exports allowed**: false (Houston-first policy)" >> $GITHUB_STEP_SUMMARY