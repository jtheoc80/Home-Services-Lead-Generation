name: TX Permits Ingestion Pipeline

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      sources:
        description: 'Comma-separated list of sources to ingest (default: all)'
        required: false
        default: 'dallas_permits,austin_permits,arlington_permits'
      full_refresh:
        description: 'Force full refresh (ignore incremental state)'
        required: false
        default: 'false'
        type: boolean
  
  # Nightly schedule (5 AM UTC = 11 PM CST)
  schedule:
    - cron: '0 5 * * *'

env:
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  SODA_APP_TOKEN: ${{ secrets.SODA_APP_TOKEN }}
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
  USE_WEATHER: false

# Prevent overlapping runs on self-hosted runners
concurrency:
  group: scrape-${{ github.ref }}
  cancel-in-progress: false

jobs:
  ingest-tx-permits:
    runs-on: [self-hosted, linux, x64, scrape]
    timeout-minutes: 30
    defaults:
      run:
        working-directory: pipelines
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: "Preflight: verify required secrets"
      id: preflight
      shell: bash
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SODA_APP_TOKEN: ${{ secrets.SODA_APP_TOKEN }}
      run: |
        ok=1
        for v in DATABASE_URL SODA_APP_TOKEN; do
          val="${!v}"
          if [ -z "$val" ]; then
            echo "::error title=$v missing::Set $v in GitHub → Settings → Secrets and variables → Actions"
            ok=0
          else
            echo "::add-mask::$val"
          fi
        done
        echo "ok=$ok" >> "$GITHUB_OUTPUT"
        [ "$ok" -eq 1 ]
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: '1.7.1'
        virtualenvs-create: true
        virtualenvs-in-project: true
        
    - name: Cache Poetry dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
        
    - name: Install dependencies
      run: |
        cd ..
        poetry install --no-dev
      
    - name: Apply database migrations
      run: |
        cd ..
        poetry run make db-migrate
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Run raw data ingestion
      id: load_raw
      run: |
        # Set defaults in bash for cron-safe execution
        SOURCES="${{ inputs.sources }}"
        [ -z "$SOURCES" ] && SOURCES=""
        FULL_REFRESH="${{ inputs.full_refresh }}"
        [ -z "$FULL_REFRESH" ] && FULL_REFRESH="false"
        
        if [ "$SOURCES" != "" ]; then
          SOURCES_ARG="--only $SOURCES"
        else
          SOURCES_ARG="--only dallas_permits,austin_permits,arlington_permits"
        fi
        
        if [ "$FULL_REFRESH" == "true" ]; then
          SINCE_ARG="--since 2024-01-01"
        else
          SINCE_ARG=""
        fi
        
        echo "Running: poetry run python -m pipelines.load_raw $SOURCES_ARG $SINCE_ARG"
        cd .. && poetry run python -m pipelines.load_raw $SOURCES_ARG $SINCE_ARG
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SODA_APP_TOKEN: ${{ secrets.SODA_APP_TOKEN }}
        
    - name: Normalize permits data
      id: normalize
      run: |
        echo "Normalizing permit data to gold.permits schema..."
        cd .. && poetry run python -m pipelines.normalize_permits
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Build leads from fresh permits
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        echo "Building leads from fresh permits (last 7 days)..."
        if [ -n "$SUPABASE_URL" ] && [ -n "$SUPABASE_SERVICE_ROLE_KEY" ]; then
          response=$(curl -sS "$SUPABASE_URL/rest/v1/rpc/upsert_leads_from_permits" \
            -H "apikey: $SUPABASE_SERVICE_ROLE_KEY" \
            -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
            -H "Content-Type: application/json" \
            -d '{"p_days": 7}')
          echo "Lead building response: $response"
          
          # Extract counts from response if possible
          if echo "$response" | grep -q "inserted_count\|updated_count\|total_processed"; then
            echo "✅ Successfully built leads from fresh permits"
          else
            echo "⚠️  Lead building completed but response format unexpected"
          fi
        else
          echo "⚠️  Supabase credentials not available, skipping lead building"
        fi
        
    - name: Publish lead scores
      id: publish
      run: |
        echo "Computing and publishing lead scores..."
        cd .. && poetry run python -m pipelines.publish
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Run data quality checks
      id: quality_check
      run: |
        echo "Running Great Expectations data quality checks..."
        cd .. && poetry run python great_expectations/permits_validation.py
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      continue-on-error: true  # Don't fail pipeline on quality check failures
      
    - name: Generate sample data artifact
      run: |
        echo "Generating sample data for verification..."
        mkdir -p ../artifacts
        
        # Export sample of recent permits
        cd .. && poetry run python -c "
        import psycopg2
        import csv
        import os
        from datetime import datetime, timedelta
        
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # Get recent permits sample
        cur.execute('''
          SELECT source_id, permit_id, city, permit_type, issued_at, 
                 valuation, address_full, status, updated_at
          FROM gold.permits 
          WHERE updated_at >= %s 
          ORDER BY updated_at DESC 
          LIMIT 100
        ''', (datetime.now() - timedelta(days=1),))
        
        with open('artifacts/recent_permits_sample.csv', 'w', newline='') as f:
          writer = csv.writer(f)
          writer.writerow(['source_id', 'permit_id', 'city', 'permit_type', 'issued_at', 
                          'valuation', 'address_full', 'status', 'updated_at'])
          writer.writerows(cur.fetchall())
        
        # Get lead scores sample  
        cur.execute('''
          SELECT lead_id, version, score, created_at
          FROM gold.lead_scores 
          WHERE created_at >= %s 
          ORDER BY created_at DESC 
          LIMIT 100
        ''', (datetime.now() - timedelta(days=1),))
        
        with open('artifacts/recent_lead_scores_sample.csv', 'w', newline='') as f:
          writer = csv.writer(f)
          writer.writerow(['lead_id', 'version', 'score', 'created_at'])
          writer.writerows(cur.fetchall())
          
        conn.close()
        "
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Upload ETL artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: etl-${{ github.run_id }}
        path: |
          artifacts/**/*.csv
          logs/**/*.log
          logs/etl_output.log
        if-no-files-found: warn
        retention-days: 14
        
    - name: Upload sample data artifact
      uses: actions/upload-artifact@v4
      with:
        name: tx-permits-sample-${{ github.run_number }}
        path: ../artifacts/
        retention-days: 7
        
    - name: Post to Slack on failure
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: 'TX Permits ingestion pipeline failed. Check the workflow logs.'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
    - name: Pipeline Summary
      if: always()
      run: |
        # Set defaults using shell logic
        SOURCES="${{ inputs.sources }}"
        [ -z "$SOURCES" ] && SOURCES="dallas_permits,austin_permits,arlington_permits"
        
        FULL_REFRESH="${{ inputs.full_refresh }}"
        [ -z "$FULL_REFRESH" ] && FULL_REFRESH="false"
        
        echo "## TX Permits Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Sources**: $SOURCES" >> $GITHUB_STEP_SUMMARY
        echo "- **Full Refresh**: $FULL_REFRESH" >> $GITHUB_STEP_SUMMARY
        echo "- **Raw Data Ingestion**: ${{ steps.load_raw.outcome }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Data Normalization**: ${{ steps.normalize.outcome }}" >> $GITHUB_STEP_SUMMARY  
        echo "- **Lead Score Publishing**: ${{ steps.publish.outcome }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Data Quality Checks**: ${{ steps.quality_check.outcome }}" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "../artifacts/recent_permits_sample.csv" ]; then
          PERMIT_COUNT=$(wc -l < ../artifacts/recent_permits_sample.csv)
          echo "- **Recent Permits Processed**: $((PERMIT_COUNT - 1))" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "../artifacts/recent_lead_scores_sample.csv" ]; then
          SCORE_COUNT=$(wc -l < ../artifacts/recent_lead_scores_sample.csv)
          echo "- **Lead Scores Generated**: $((SCORE_COUNT - 1))" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Sample data artifact: \`tx-permits-sample-${{ github.run_number }}\`" >> $GITHUB_STEP_SUMMARY