name: TX Permits Ingestion Pipeline

on:
  # Manual trigger
  workflow_dispatch:
    inputs:
      sources:
        description: 'Comma-separated list of sources to ingest (default: all)'
        required: false
        default: 'dallas_permits,austin_permits,arlington_permits'
      full_refresh:
        description: 'Force full refresh (ignore incremental state)'
        required: false
        default: 'false'
        type: boolean
  
  # Nightly schedule (5 AM UTC = 11 PM CST)
  schedule:
    - cron: '0 5 * * *'

env:
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  SODA_APP_TOKEN: ${{ secrets.SODA_APP_TOKEN }}
  USE_WEATHER: false

jobs:
  ingest-tx-permits:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    defaults:
      run:
        working-directory: pipelines
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: '1.7.1'
        virtualenvs-create: true
        virtualenvs-in-project: true
        
    - name: Cache Poetry dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: venv-${{ runner.os }}-${{ hashFiles('**/poetry.lock') }}
        
    - name: Install dependencies
      run: |
        cd ..
        poetry install --no-dev
      
    - name: Apply database migrations
      run: |
        cd ..
        poetry run make db-migrate
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Run raw data ingestion
      id: load_raw
      run: |
        if [ "${{ github.event.inputs.sources }}" != "" ]; then
          SOURCES_ARG="--only ${{ github.event.inputs.sources }}"
        else
          SOURCES_ARG="--only dallas_permits,austin_permits,arlington_permits"
        fi
        
        if [ "${{ github.event.inputs.full_refresh }}" == "true" ]; then
          SINCE_ARG="--since 2024-01-01"
        else
          SINCE_ARG=""
        fi
        
        echo "Running: poetry run python -m pipelines.load_raw $SOURCES_ARG $SINCE_ARG"
        cd .. && poetry run python -m pipelines.load_raw $SOURCES_ARG $SINCE_ARG
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        SODA_APP_TOKEN: ${{ secrets.SODA_APP_TOKEN }}
        
    - name: Normalize permits data
      id: normalize
      run: |
        echo "Normalizing permit data to gold.permits schema..."
        cd .. && poetry run python -m pipelines.normalize_permits
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Publish lead scores
      id: publish
      run: |
        echo "Computing and publishing lead scores..."
        cd .. && poetry run python -m pipelines.publish
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Run data quality checks
      id: quality_check
      run: |
        echo "Running Great Expectations data quality checks..."
        cd .. && poetry run python great_expectations/permits_validation.py
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
      continue-on-error: true  # Don't fail pipeline on quality check failures
      
    - name: Generate sample data artifact
      run: |
        echo "Generating sample data for verification..."
        mkdir -p ../artifacts
        
        # Export sample of recent permits
        cd .. && poetry run python -c "
        import psycopg2
        import csv
        import os
        from datetime import datetime, timedelta
        
        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        cur = conn.cursor()
        
        # Get recent permits sample
        cur.execute('''
          SELECT source_id, permit_id, city, permit_type, issued_at, 
                 valuation, address_full, status, updated_at
          FROM gold.permits 
          WHERE updated_at >= %s 
          ORDER BY updated_at DESC 
          LIMIT 100
        ''', (datetime.now() - timedelta(days=1),))
        
        with open('artifacts/recent_permits_sample.csv', 'w', newline='') as f:
          writer = csv.writer(f)
          writer.writerow(['source_id', 'permit_id', 'city', 'permit_type', 'issued_at', 
                          'valuation', 'address_full', 'status', 'updated_at'])
          writer.writerows(cur.fetchall())
        
        # Get lead scores sample  
        cur.execute('''
          SELECT lead_id, version, score, created_at
          FROM gold.lead_scores 
          WHERE created_at >= %s 
          ORDER BY created_at DESC 
          LIMIT 100
        ''', (datetime.now() - timedelta(days=1),))
        
        with open('artifacts/recent_lead_scores_sample.csv', 'w', newline='') as f:
          writer = csv.writer(f)
          writer.writerow(['lead_id', 'version', 'score', 'created_at'])
          writer.writerows(cur.fetchall())
          
        conn.close()
        "
      env:
        DATABASE_URL: ${{ secrets.DATABASE_URL }}
        
    - name: Upload ETL artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: etl-artifacts-${{ github.run_id }}
        path: |
          artifacts/**/*.csv
          logs/**/*.log
          logs/etl_output.log
        if-no-files-found: warn
        
    - name: Upload sample data artifact
      uses: actions/upload-artifact@v4
      with:
        name: tx-permits-sample-${{ github.run_number }}
        path: ../artifacts/
        retention-days: 7
        
    - name: Post to Slack on failure
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        text: 'TX Permits ingestion pipeline failed. Check the workflow logs.'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        
    - name: Pipeline Summary
      if: always()
      run: |
        # Set defaults using shell logic instead of logical OR
        SOURCES="${{ github.event.inputs.sources }}"
        [ -z "$SOURCES" ] && SOURCES="dallas_permits,austin_permits,arlington_permits"
        
        FULL_REFRESH="${{ github.event.inputs.full_refresh }}"
        [ -z "$FULL_REFRESH" ] && FULL_REFRESH="false"
        
        echo "## TX Permits Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Sources**: $SOURCES" >> $GITHUB_STEP_SUMMARY
        echo "- **Full Refresh**: $FULL_REFRESH" >> $GITHUB_STEP_SUMMARY
        echo "- **Raw Data Ingestion**: ${{ steps.load_raw.outcome }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Data Normalization**: ${{ steps.normalize.outcome }}" >> $GITHUB_STEP_SUMMARY  
        echo "- **Lead Score Publishing**: ${{ steps.publish.outcome }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Data Quality Checks**: ${{ steps.quality_check.outcome }}" >> $GITHUB_STEP_SUMMARY
        
        if [ -f "../artifacts/recent_permits_sample.csv" ]; then
          PERMIT_COUNT=$(wc -l < ../artifacts/recent_permits_sample.csv)
          echo "- **Recent Permits Processed**: $((PERMIT_COUNT - 1))" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -f "../artifacts/recent_lead_scores_sample.csv" ]; then
          SCORE_COUNT=$(wc -l < ../artifacts/recent_lead_scores_sample.csv)
          echo "- **Lead Scores Generated**: $((SCORE_COUNT - 1))" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Sample data artifact: \`tx-permits-sample-${{ github.run_number }}\`" >> $GITHUB_STEP_SUMMARY