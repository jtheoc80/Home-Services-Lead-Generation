name: City of Houston ETL
on:
  schedule: [{ cron: "0 6 * * *" }]
  workflow_dispatch:
    inputs:
      include_archives:
        description: 'Include archives backfill (last 12 weeks)'
        required: false
        default: 'false'
        type: boolean
      archive_weeks:
        description: 'Number of weeks to backfill from archives'
        required: false
        default: '12'
        type: string

permissions: { contents: read }

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run:
    runs-on: [self-hosted, linux, x64, scrape]
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      HOUSTON_WEEKLY_XLSX_URL: ${{ secrets.HOUSTON_WEEKLY_XLSX_URL }}
      HOUSTON_SOLD_PERMITS_URL: ${{ secrets.HOUSTON_SOLD_PERMITS_URL }}
      DAYS: ${{ vars.DAYS || '7' }}
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-node@v4
        with: 
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      - run: npm run pw:install
      
      - name: Sanity check required env
        run: |
          set -euo pipefail
          missing=0
          for v in SUPABASE_URL SUPABASE_SERVICE_ROLE_KEY USER_AGENT; do
            if [ -z "${!v:-}" ]; then echo "MISSING $v"; missing=1; fi
          done
          # Add city-specific vars below (uncomment what applies)
          # Dallas
          # for v in DALLAS_ARCGIS_URL; do [ -z "${!v:-}" ] && echo "MISSING $v" && missing=1; done
          # Austin
          # for v in AUSTIN_SODA_APP_TOKEN AUSTIN_DATASET_ID; do [ -z "${!v:-}" ] && echo "MISSING $v" && missing=1; done
          # San Antonio
          # for v in SANANTONIO_SODA_APP_TOKEN SAN_ANTONIO_DATASET_ID; do [ -z "${!v:-}" ] && echo "MISSING $v" && missing=1; done
          # Houston
          for v in HOUSTON_WEEKLY_XLSX_URL HOUSTON_SOLD_PERMITS_URL; do [ -z "${!v:-}" ] && echo "MISSING $v" && missing=1; done
          exit $missing
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          USER_AGENT: ${{ secrets.USER_AGENT || 'LeadLedgerETL/1.0' }}
          HOUSTON_WEEKLY_XLSX_URL: ${{ secrets.HOUSTON_WEEKLY_XLSX_URL }}
          HOUSTON_SOLD_PERMITS_URL: ${{ secrets.HOUSTON_SOLD_PERMITS_URL }}

      - name: Preflight connectivity
        run: |
          set -euxo pipefail
          # Un-comment the checks you need. We print only the status line.
          # Dallas ArcGIS:
          # curl -sS --get "${DALLAS_ARCGIS_URL}" \
          #   --data-urlencode 'where=1=1' \
          #   --data-urlencode 'outFields=*' \
          #   --data-urlencode 'resultRecordCount=1' \
          #   --data-urlencode 'f=json' | head -c 200 | tr -d '\n' && echo

          # Austin Socrata:
          # curl -sS -H "X-App-Token: ${AUSTIN_SODA_APP_TOKEN}" \
          #   "https://data.austintexas.gov/resource/${AUSTIN_DATASET_ID}.json?\$limit=1" | head -c 200 | tr -d '\n' && echo

          # San Antonio Socrata:
          # curl -sS -H "X-App-Token: ${SANANTONIO_SODA_APP_TOKEN}" \
          #   "https://data.sanantonio.gov/resource/${SAN_ANTONIO_DATASET_ID}.json?\$limit=1" | head -c 200 | tr -d '\n' && echo

          # Houston weekly/sold landing pages:
          curl -sS -I "${HOUSTON_WEEKLY_XLSX_URL}" | head -n1
          curl -sS -I "${HOUSTON_SOLD_PERMITS_URL}" | head -n1
        env:
          HOUSTON_WEEKLY_XLSX_URL: ${{ secrets.HOUSTON_WEEKLY_XLSX_URL }}
          HOUSTON_SOLD_PERMITS_URL: ${{ secrets.HOUSTON_SOLD_PERMITS_URL }}
          
      - name: Connectivity check (Houston weekly page)
        run: |
          set -Eeuo pipefail
          URL="https://www.houstontx.gov/planning/DevelopRegs/dev_reports.html"
          echo "Checking $URL"
          getent ahosts www.houstontx.gov || true
          STATUS=$(curl -sS -I -A "LeadLedgerETL/1.0 (+github-actions)" -o /dev/null -w "%{http_code}" "$URL" || true)
          echo "HTTP_STATUS=$STATUS"
          test "$STATUS" = "200" || { echo "❌ Houston page unreachable ($STATUS)"; exit 1; }

          
      # Download weekly (or sold)
      - name: Houston weekly download
        env:
          HOUSTON_WEEKLY_URL: ${{ secrets.HOUSTON_WEEKLY_URL }}
          HOUSTON_SOLD_URL:   ${{ secrets.HOUSTON_SOLD_URL }}
          USER_AGENT:         ${{ secrets.USER_AGENT }}
          OUT_DIR:            artifacts/houston
        run: npm run dl:houston:weekly

      # Optional: parse + upsert in Node (skip if your Python ETL handles it)
      - name: Parse + upsert to Supabase (Node)
        if: ${{ env.SUPABASE_URL && env.SUPABASE_SERVICE_ROLE_KEY }}
        env:
          SUPABASE_URL:              ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          OUT_DIR:                   artifacts/houston
        run: npx tsx scripts/houston_parse_and_upsert.ts
        

      
      # Install browsers + OS deps (works with yum/dnf on Amazon Linux 2023)
      - run: npx playwright install --with-deps chromium

      # Download the XLSX (weekly or sold)
      - name: Houston weekly download
        env:
          HOUSTON_WEEKLY_URL: ${{ secrets.HOUSTON_WEEKLY_XLSX_URL }}
          HOUSTON_SOLD_URL:   ${{ secrets.HOUSTON_SOLD_PERMITS_URL }}
          USER_AGENT:         ${{ secrets.USER_AGENT || 'LeadLedgerETL/1.0' }}
          OUT_DIR:            artifacts/houston
        run: npx tsx scripts/houston_download.ts weekly

      # (Optional) upload file as artifact for debugging
      - uses: actions/upload-artifact@v4
        with:
          name: houston-xlsx
          path: artifacts/houston/*.xls*
          if-no-files-found: error

      - name: Ingest → Upsert → Build Leads
        run: |
          mkdir -p logs artifacts
          npm run ingest:coh 2>&1 | tee logs/etl_output.log

      - name: Summary
        if: always()
        run: |
          echo "### COH ETL" >> "$GITHUB_STEP_SUMMARY"
          if test -f logs/etl-summary.json; then
            cat logs/etl-summary.json >> "$GITHUB_STEP_SUMMARY"
          fi
      # Upload the file(s) for debugging
      - uses: actions/upload-artifact@v4
        if: always()
        with:
          name: houston-xlsx-${{ github.run_id }}
          path: artifacts/houston/*.xls*
          if-no-files-found: warn
          
      - name: Upload ETL logs (always)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: etl-logs-${{ github.job }}-${{ github.run_id }}
          path: |
            logs/**/*.log
            artifacts/**/*
          if-no-files-found: warn

  backfill-archives:
    runs-on: [self-hosted, linux, x64, scrape]
    if: github.event.inputs.include_archives == 'true'
    needs: run
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      ARCHIVE_WEEKS: ${{ github.event.inputs.archive_weeks || '12' }}
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-node@v4
        with: 
          node-version: '20'
          cache: 'npm'
      - run: npm ci
      - name: Backfill Houston archives (last ${{ github.event.inputs.archive_weeks || '12' }} weeks)
        run: |
          node -v
          npx tsx scripts/ingest-coh-archives.ts

      - name: Archives Summary
        if: always()
        run: |
          echo "### Houston Archives Backfill" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Weeks processed**: ${{ github.event.inputs.archive_weeks || '12' }}" >> "$GITHUB_STEP_SUMMARY"
          echo "- **Status**: ${{ job.status == 'success' && '✅ Success' || '❌ Failed' }}" >> "$GITHUB_STEP_SUMMARY"